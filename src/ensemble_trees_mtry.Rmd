---
title: "Ensemble_Trees"
author: "Brandon Neo Bing Jie"
date: "2025-04-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# renv::init()
```


```{r}
library(randomForest) # for random forest
```

```{r}
df_cleaned <- read.csv("../data/processed_hdb_data.csv")

```

# Prepare Data

```{r}

# ---- Prepare data ----
set.seed(123)
X <- model.matrix(resale_price ~ . -1, data = df_cleaned)
Y <- df_cleaned$resale_price
n <- nrow(X)
train_idx <- sample(1:n, size = 0.8 * n)
X_train <- X[train_idx, ]
Y_train <- Y[train_idx]
X_test <- X[-train_idx, ]
Y_test <- Y[-train_idx]

# ---- R² helper ----
rsq <- function(actual, predicted) {
  1 - sum((actual - predicted)^2) / sum((actual - mean(actual))^2)
}
```

# Normalise the numerical features

```{r}

colnames(X_train)

# might be incomplete - but not needed for RF, needed for the other linear models for scaling
numerical_features = c("floor_area_sqm", "count_mrt_stations_within_1km", "distance_to_cbd", "count_pri_schs_within_1km",
                       "count_hawkers_within_1km", "remaining_lease_months", "level")
```



```{r}
# 2.1. Prepare a single data.frame for caret
train_df <- data.frame(resale_price = Y_train, X_train)

library(caret)
set.seed(123)

# 2.2. Define cross‐validation strategy
ctrl <- trainControl(
  method           = "cv", # "repeatedcv"
  number           = 5,       # 5‐fold CV
  # repeats          = 2,       # repeat twice
  search           = "grid"
)

# 2.3. Define a tuning grid for mtry (you can also include ntree or nodesize by wrapping randomForest in a custom model)
mtry_vals <- seq( floor(sqrt(ncol(X_train))) - 5,
                  floor(sqrt(ncol(X_train))) + 5,
                  by = 2 )
mtry_vals

grid <- expand.grid(mtry = mtry_vals)
grid

# 2.4. Run the grid search
rf_tuned <- train(
  resale_price ~ .,
  data       = train_df,
  method     = "rf",
  metric     = "RMSE",    # or "Rsquared"
  tuneGrid   = grid,
  trControl  = ctrl,
  ntree      = 500            # you can also try different values here
)

# 2.5. Inspect results
print(rf_tuned)
```

```{r}
plot(rf_tuned)    # show performance vs. mtry


```


```{r}
# 3.1. Extract the best mtry
best_mtry_caret <- rf_tuned$bestTune$mtry
best_mtry_caret
```


```{r}
# 3.2. Refit on full training set
rf_final <- randomForest(
  x     = X_train,
  y     = Y_train,
  mtry  = best_mtry_caret,
  ntree = 500           # increase if you want more stability, set 1000
)

# 3.3. Predict and compute R² on the test set
preds <- predict(rf_final, X_test)
preds

```

```{r}
library(Metrics)

# Compute OOS metrics
rf_results <- data.frame(
  Model = "Random Forest",
  RMSE = round(rmse(Y_test, preds), 5),
  MAE  = round(mae(Y_test, preds), 5),
  MAPE = round(mape(Y_test, preds) * 100, 5),
  R2   = round(rsq(Y_test, preds), 5)
)

rf_results

```


```{r}

rf_preds_df <- data.frame(RF_predictions = preds)  # Custom column name

write.csv(rf_preds_df, "../results/random_forest_oos_predictions.csv", row.names = FALSE)


```